{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Hyperparameter tuning: lung cancer detection (AI+Health SIG)",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r26fefWKK1vf",
        "outputId": "15551b8d-1003-4b38-c83d-93daa0ebb012"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydicom\n",
            "  Downloading pydicom-2.3.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 5.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-2.3.0\n",
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from python-docx) (4.2.6)\n",
            "Building wheels for collected packages: python-docx\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184507 sha256=3b4fef15fc3ab0187adadcb0b4a3821577cf313c4a8c517317e16139aef615b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n",
            "Successfully built python-docx\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-0.8.11\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.colab import drive\n",
        "import numpy as np \n",
        "!pip install pydicom\n",
        "import pydicom \n",
        "import matplotlib.pyplot as plt\n",
        "!pip install python-docx\n",
        "import docx \n",
        "import pandas as pd \n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# np.save(r'/content/drive/My Drive/images.npy', images)\n",
        "# np.save(r'/content/drive/My Drive/labels.npy', labels)\n",
        "# np.save(r'/content/drive/My Drive/patient_IDs.npy', patient_IDs)\n",
        "\n",
        "x = np.load(r'/content/drive/My Drive/images.npy')\n",
        "y = np.load(r'/content/drive/My Drive/labels.npy')\n",
        "patient_IDs = np.load(r'/content/drive/My Drive/patient_IDs.npy')\n",
        "\n",
        "## include talking about how our dataset doesn't have enough patients to test on all stages\n",
        "    # only test on the data with enough patients to be learnable from and include a disclaimer \n",
        "# print(len(patient_IDs))"
      ],
      "metadata": {
        "id": "eG1gY2i7wkWQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.unique(patient_IDs).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrpl1BUdS1cE",
        "outputId": "94046608-087c-4729-9ce1-52258e320d35"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(61,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indices_to_drop = []\n",
        "\n",
        "for i in range(len(y)):\n",
        "    # print(y)\n",
        "    if y[i] == 'N/A' or y[i] == '4':\n",
        "        indices_to_drop.append(i)\n",
        "        # print('true')\n",
        "\n",
        "print('len x', len(x))\n",
        "x = [x[i] for i in range(len(x)) if i not in indices_to_drop]\n",
        "y = [y[i] for i in range(len(y)) if i not in indices_to_drop]\n",
        "patient_IDs = [patient_IDs[i] for i in range(len(patient_IDs)) if i not in indices_to_drop]\n",
        "# print('len x', len(x))\n",
        "\n",
        "# print(np.unique(y, return_counts=True))\n",
        "# print(np.unique(patient_IDs, return_counts=True))\n",
        "\n",
        "y_new = []\n",
        "\n",
        "for i in range(len(y)):\n",
        "    if y[i] == '1A' or y[i] == '1B':\n",
        "        y_new.append('1')\n",
        "    elif y[i] == '2A' or y[i] == '2B':\n",
        "        y_new.append('2')\n",
        "    elif y[i] == '3A' or y[i] == '3B' or y[i] == '3':\n",
        "        y_new.append('3')\n",
        "    elif y[i] == '4':\n",
        "        y_new.append('4')\n",
        "    else:\n",
        "        print(y_new)\n",
        "    #     y_new.append('N/A')\n",
        "\n",
        "# print('len y', len(y))\n",
        "# print(\"len y_new\", len(y_new))\n",
        "\n",
        "\n",
        "# (array(['1A', '1B', '2A', '2B', '3A', '3B', 'N/A'], dtype='<U3'), array([1230,  419,  301, 1182,  785,  308,  337]))\n",
        "y = y_new\n",
        "\n",
        "\n",
        "# print(np.count(y, return_counts=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ai8pQBkeNaFd",
        "outputId": "57578090-c562-4da8-a874-4f2b065b9ee8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len x 4686\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x), len(y), len(patient_IDs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STrTyldHVYVe",
        "outputId": "b4f289ff-b47f-4eea-97f2-0bd2d94c6d46"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4287 4287 4287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# groups = [int(patient) for patient in data['patient']]\n",
        "groups = []\n",
        "# data[0:0]\n",
        "\n",
        "groups = [int(patient) for patient in patient_IDs]\n",
        "\n",
        "# groups.append\n",
        "# for index, row in data.iterrows():\n",
        "    #  groups.append(int(row['patient']))\n",
        "#     print(row)\n",
        "#     groups.append(int(row['patient']))\n",
        "# groupings by patient ID \n",
        "# please see: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupShuffleSplit.html"
      ],
      "metadata": {
        "id": "vzQEiakKXGHs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(groups)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XT1pOBbWYhEh",
        "outputId": "c24a319b-3477-4b71-d979-5118d415a2a4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 172, 172, 172, 172, 172, 172, 172, 172, 172, 172, 172, 172, 172, 172, 172, 172, 172, 172, 172, 172, 172, 172, 172, 172, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Grouping code \n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
        "\n",
        "## Create our training, validation, testing datasets (70-15-15 split)\n",
        "# [[list of all patient 1], [patient 2], [etc]] ## labels \n",
        "# [[patient 21], [patient 3]]\n",
        "\n",
        "# 1 label, 1 patient ID, [all of the images]\n",
        "\n",
        "\n",
        "# x = images \n",
        "# y = labels\n",
        "patient_id = patient_IDs\n",
        "\n",
        "# GroupShuffleSplit\n",
        "gss = ShuffleSplit(n_splits=1, train_size=.7, random_state=60)\n",
        "gss.get_n_splits()\n",
        "\n",
        "# train_idx, test_idx = gss.split(x, y, groups)\n",
        "for train_idx, test_idx in gss.split(x, y):\n",
        "    train_idx = train_idx\n",
        "    test_idx = test_idx\n",
        "    # print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n",
        "\n",
        "## Temporarily split into train and test before doing a test/validation split \n",
        "print(len(x), len(train_idx))\n",
        "\n",
        "\n",
        "# x, y, patient_id = np.array(x), np.array(y), np.array(patient_id)\n",
        "print('line 0')\n",
        "x, y, patient_id = np.array(x), np.array(y), np.array(patient_id)\n",
        "print(\"line 1\")\n",
        "x_train = [x[i] for i in train_idx]\n",
        "y_train = [y[i] for i in train_idx]\n",
        "patient_id_train = [patient_id[i] for i in train_idx]\n",
        "print('done')\n",
        "# x_train, y_train, patient_id_train = [x[i] for i in train_idx], [y[i] for i in train_idx], [patient_id[i] for i in train_idx]\n",
        "# x_train = x[train_idx]\n",
        "print(\"line2\")\n",
        "\n",
        "# x_train, y_train, patient_id_train = np.array(x)[train_idx], np.array(y)[train_idx], np.array(patient_id)[train_idx]\n",
        "print('line 3')\n",
        "### gets to this point ^^\n",
        "\n",
        "x_test_temp, y_test_temp, patient_id_test_temp = [x[i] for i in test_idx], [y[i] for i in test_idx], [patient_id[i] for i in test_idx]\n",
        "print(len(x_test_temp))\n",
        "groups_test = [groups[i] for i in test_idx]\n",
        "\n",
        "\n",
        "gss = ShuffleSplit(n_splits=1, train_size=.5, random_state=27) # was 43\n",
        "gss.get_n_splits()\n",
        "\n",
        "for test_idx, valid_idx in gss.split(x_test_temp, y_test_temp):\n",
        "    test_idx = test_idx\n",
        "    valid_idx = valid_idx \n",
        "\n",
        "x_valid, x_test = [list(x_test_temp)[i] for i in valid_idx], [list(x_test_temp)[i] for i in test_idx]\n",
        "y_valid, y_test = [list(y_test_temp)[i] for i in valid_idx], [list(y_test_temp)[i] for i in test_idx]\n",
        "patient_id_valid, patient_id_test = [list(patient_id_test_temp)[i] for i in valid_idx], [list(patient_id_test_temp)[i] for i in test_idx]\n",
        "\n",
        "print(len(x_train), len(x_valid), len(x_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXjW-ozSNWi1",
        "outputId": "169128fb-e0d5-4d46-8771-d61f1daf5b03"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "4287 3000\n",
            "line 0\n",
            "line 1\n",
            "done\n",
            "line2\n",
            "line 3\n",
            "1287\n",
            "3000 644 643\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model code"
      ],
      "metadata": {
        "id": "2xsdMrg1GCtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras import Sequential \n",
        "from tensorflow.keras.layers import Conv2D, Activation, MaxPooling2D, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# ^^ transfer learning using resnet 50???"
      ],
      "metadata": {
        "id": "H_LesmL5nKC_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 30 # was 30 \n",
        "BS = 16\n",
        "INIT_LR = 1e-4\n",
        "\n",
        "# Beginning: 7, 32, 0.001\n",
        "# End (after tuning): , , "
      ],
      "metadata": {
        "id": "BjcJORK4n4vT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## tf model \n",
        "height, width, depth = 512, 512, 1\n",
        "n_classes = 3\n",
        "model = Sequential()\n",
        "inputShape = (height, width, depth)\n",
        "chanDim = -1\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding=\"same\", input_shape=inputShape, activation='relu'))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), padding=\"same\",activation='relu'))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(3,3)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(n_classes))\n",
        "model.add(Activation(\"softmax\"))"
      ],
      "metadata": {
        "id": "SpXh_yLvSaMT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn \n",
        "from sklearn.preprocessing import OneHotEncoder, LabelBinarizer\n",
        "# LabelBinarizer"
      ],
      "metadata": {
        "id": "D6DABq-zE5Jz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(y_train)\n",
        "y_train_bin = LabelBinarizer().fit_transform(y_train)\n",
        "y_test_bin = LabelBinarizer().fit_transform(y_test)\n",
        "y_valid_bin = LabelBinarizer().fit_transform(y_valid)\n",
        "y_valid = y_valid_bin\n",
        "y_train = y_train_bin\n",
        "y_test = y_test_bin"
      ],
      "metadata": {
        "id": "CeuH0jq3E_mo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.array(x_train, dtype=np.uint8).shape)\n",
        "\n",
        "print(np.unique(y_test))\n",
        "print(np.unique(y_train))\n",
        "\n",
        "x_train = np.array(x_train, dtype=np.uint8)\n",
        "y_train = np.array(y_train)\n",
        "x_test = np.array(x_test, dtype=np.uint8)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kzko8EPYoaWU",
        "outputId": "dcf760f4-6b68-48cf-8a6e-21e0919db023"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3000, 512, 512)\n",
            "[0 1]\n",
            "[0 1]\n",
            "(3000, 512, 512) (3000, 3)\n",
            "(643, 512, 512) (643, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = np.concatenate([np.array(x_test), np.array(x_valid)])\n",
        "print(x_test.shape)\n",
        "y_test = np.concatenate([np.array(y_test), np.array(y_valid)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyWP87aeAMcg",
        "outputId": "884eea5a-0d19-44da-de07-e3c1ff1c6aad"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1287, 512, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train the network\n",
        "print(\"[INFO] training network...\")\n",
        "model.compile(loss=\"categorical_crossentropy\", \n",
        "              optimizer=Adam(learning_rate=INIT_LR), \n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(x_train, y_train, batch_size=BS,\n",
        "                    validation_data=(np.array(x_test), np.array(y_test)), #was np.array(x_valid)\n",
        "                    epochs=EPOCHS, verbose=1, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0DhedL7n9y1",
        "outputId": "a14a4534-7b68-47d5-99c1-1a76594cc3f8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] training network...\n",
            "Epoch 1/30\n",
            "188/188 [==============================] - 73s 316ms/step - loss: 31.7083 - accuracy: 0.6307 - val_loss: 31.3607 - val_accuracy: 0.6939\n",
            "Epoch 2/30\n",
            "188/188 [==============================] - 56s 298ms/step - loss: 0.8684 - accuracy: 0.8833 - val_loss: 39.7467 - val_accuracy: 0.7591\n",
            "Epoch 3/30\n",
            "188/188 [==============================] - 56s 298ms/step - loss: 0.1550 - accuracy: 0.9637 - val_loss: 44.6249 - val_accuracy: 0.7653\n",
            "Epoch 4/30\n",
            "188/188 [==============================] - 56s 298ms/step - loss: 0.0880 - accuracy: 0.9787 - val_loss: 58.1091 - val_accuracy: 0.7350\n",
            "Epoch 5/30\n",
            "188/188 [==============================] - 56s 299ms/step - loss: 0.0614 - accuracy: 0.9830 - val_loss: 57.4077 - val_accuracy: 0.7545\n",
            "Epoch 6/30\n",
            "188/188 [==============================] - 56s 299ms/step - loss: 0.0321 - accuracy: 0.9897 - val_loss: 54.6528 - val_accuracy: 0.7591\n",
            "Epoch 7/30\n",
            "188/188 [==============================] - 56s 298ms/step - loss: 0.0302 - accuracy: 0.9917 - val_loss: 58.1866 - val_accuracy: 0.7801\n",
            "Epoch 8/30\n",
            "188/188 [==============================] - 56s 298ms/step - loss: 0.0474 - accuracy: 0.9913 - val_loss: 60.0066 - val_accuracy: 0.7770\n",
            "Epoch 9/30\n",
            "188/188 [==============================] - 58s 311ms/step - loss: 0.0211 - accuracy: 0.9930 - val_loss: 59.4989 - val_accuracy: 0.7747\n",
            "Epoch 10/30\n",
            "188/188 [==============================] - 58s 311ms/step - loss: 0.0505 - accuracy: 0.9880 - val_loss: 71.3577 - val_accuracy: 0.7972\n",
            "Epoch 11/30\n",
            "188/188 [==============================] - 56s 298ms/step - loss: 0.0270 - accuracy: 0.9927 - val_loss: 73.0775 - val_accuracy: 0.7747\n",
            "Epoch 12/30\n",
            "188/188 [==============================] - 56s 297ms/step - loss: 0.0111 - accuracy: 0.9960 - val_loss: 69.7756 - val_accuracy: 0.7832\n",
            "Epoch 13/30\n",
            "188/188 [==============================] - 56s 298ms/step - loss: 0.0237 - accuracy: 0.9927 - val_loss: 77.8051 - val_accuracy: 0.7855\n",
            "Epoch 14/30\n",
            "188/188 [==============================] - 58s 311ms/step - loss: 0.0233 - accuracy: 0.9947 - val_loss: 77.3287 - val_accuracy: 0.7700\n",
            "Epoch 15/30\n",
            "188/188 [==============================] - 56s 298ms/step - loss: 0.0209 - accuracy: 0.9950 - val_loss: 66.4686 - val_accuracy: 0.7894\n",
            "Epoch 16/30\n",
            "188/188 [==============================] - 56s 297ms/step - loss: 0.0138 - accuracy: 0.9967 - val_loss: 70.7473 - val_accuracy: 0.8026\n",
            "Epoch 17/30\n",
            "188/188 [==============================] - 56s 297ms/step - loss: 0.0057 - accuracy: 0.9980 - val_loss: 62.1150 - val_accuracy: 0.7980\n",
            "Epoch 18/30\n",
            "188/188 [==============================] - 56s 297ms/step - loss: 4.7124e-04 - accuracy: 1.0000 - val_loss: 66.1725 - val_accuracy: 0.7949\n",
            "Epoch 19/30\n",
            "188/188 [==============================] - 56s 297ms/step - loss: 0.0011 - accuracy: 0.9993 - val_loss: 64.7806 - val_accuracy: 0.8104\n",
            "Epoch 20/30\n",
            "188/188 [==============================] - 56s 297ms/step - loss: 0.0013 - accuracy: 0.9993 - val_loss: 67.5982 - val_accuracy: 0.8073\n",
            "Epoch 21/30\n",
            "188/188 [==============================] - 56s 297ms/step - loss: 0.0245 - accuracy: 0.9930 - val_loss: 89.2816 - val_accuracy: 0.8026\n",
            "Epoch 22/30\n",
            "188/188 [==============================] - 56s 297ms/step - loss: 0.0313 - accuracy: 0.9917 - val_loss: 119.9813 - val_accuracy: 0.7832\n",
            "Epoch 23/30\n",
            "188/188 [==============================] - 56s 297ms/step - loss: 0.0457 - accuracy: 0.9907 - val_loss: 111.4809 - val_accuracy: 0.7894\n",
            "Epoch 24/30\n",
            "188/188 [==============================] - 56s 298ms/step - loss: 0.0022 - accuracy: 0.9993 - val_loss: 119.2715 - val_accuracy: 0.7848\n",
            "Epoch 25/30\n",
            "188/188 [==============================] - 56s 297ms/step - loss: 0.0093 - accuracy: 0.9970 - val_loss: 115.3499 - val_accuracy: 0.7855\n",
            "Epoch 26/30\n",
            "188/188 [==============================] - 58s 310ms/step - loss: 0.0110 - accuracy: 0.9970 - val_loss: 113.6546 - val_accuracy: 0.7801\n",
            "Epoch 27/30\n",
            "188/188 [==============================] - 56s 297ms/step - loss: 0.0108 - accuracy: 0.9970 - val_loss: 95.7161 - val_accuracy: 0.8159\n",
            "Epoch 28/30\n",
            "188/188 [==============================] - 56s 297ms/step - loss: 0.0173 - accuracy: 0.9973 - val_loss: 114.0855 - val_accuracy: 0.8221\n",
            "Epoch 29/30\n",
            "188/188 [==============================] - 56s 297ms/step - loss: 0.0060 - accuracy: 0.9980 - val_loss: 136.0976 - val_accuracy: 0.7995\n",
            "Epoch 30/30\n",
            "188/188 [==============================] - 56s 297ms/step - loss: 0.0062 - accuracy: 0.9970 - val_loss: 113.3444 - val_accuracy: 0.7980\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds = model.predict(np.array(x_test))\n",
        "test_preds = np.argmax(test_preds, axis=1)\n",
        "y_test_calc = np.argmax(y_test, axis=1)\n",
        "print(test_preds)\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Test accuracy: \", accuracy_score(y_test_calc, test_preds))\n"
      ],
      "metadata": {
        "id": "Rqs1MLdgmVE-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54489c3e-34a8-4411-cbfb-b2e600c4578c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 ... 0 2 2]\n",
            "Test accuracy:  0.797979797979798\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RghuRrqrHJsJ"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}