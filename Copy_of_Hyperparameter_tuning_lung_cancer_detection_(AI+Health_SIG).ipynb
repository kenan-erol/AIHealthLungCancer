{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Hyperparameter tuning: lung cancer detection (AI+Health SIG)",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r26fefWKK1vf",
        "outputId": "b3d48d72-4806-41a6-eb7c-aa65a1124910"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydicom\n",
            "  Downloading pydicom-2.3.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 21.3 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 10.8 MB/s eta 0:00:01\r\u001b[K     |▌                               | 30 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |▊                               | 40 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |▉                               | 51 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |█                               | 61 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 71 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 81 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 92 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 102 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 112 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██                              | 122 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 133 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 143 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 153 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 163 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 174 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███                             | 184 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 194 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 204 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 215 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 225 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 235 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████                            | 245 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 256 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 266 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 276 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 286 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 296 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████                           | 307 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 317 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 327 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 337 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 348 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 358 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████                          | 368 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 378 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 389 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 399 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 409 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 419 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████                         | 430 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 440 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 450 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 460 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 471 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 481 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████                        | 491 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 501 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 512 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 522 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 532 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 542 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 552 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 563 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 573 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 583 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 593 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 604 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 614 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 624 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 634 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 645 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 655 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 665 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 675 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 686 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 696 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 706 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 716 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 727 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 737 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 747 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 757 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 768 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 778 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 788 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 798 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 808 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 819 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 829 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 839 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 849 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 860 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 870 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 880 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 890 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 901 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 911 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 921 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 931 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 942 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 952 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 962 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 972 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 983 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 993 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 1.0 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 1.0 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 1.0 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 1.0 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.0 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 1.1 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.1 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 1.1 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.1 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 1.1 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.1 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.1 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.1 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.1 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.1 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.2 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.2 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 1.2 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.2 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.2 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.2 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.2 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.2 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.2 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.2 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.3 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.3 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.3 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.3 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.3 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.3 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.3 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.3 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.3 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.4 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.4 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.4 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.4 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.4 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.4 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.4 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.4 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.4 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.4 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.5 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.5 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.5 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.5 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.5 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.5 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.5 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.5 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.5 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.5 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.6 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.6 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.6 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.6 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.6 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.6 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.6 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.6 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.6 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.6 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.7 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.7 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.7 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.7 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.7 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.7 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.7 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.7 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.7 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.8 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.8 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.8 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.8 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.8 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.8 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.8 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.8 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.8 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.8 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.9 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.9 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.9 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.9 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.9 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.9 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.9 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.9 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.9 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.9 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 2.0 MB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.0 MB 8.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-2.3.0\n",
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from python-docx) (4.2.6)\n",
            "Building wheels for collected packages: python-docx\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184507 sha256=c762c5ab239ee666fe872dead1ab553dda693fcd574ba7c41a3936866d56bc21\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n",
            "Successfully built python-docx\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-0.8.11\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.colab import drive\n",
        "import numpy as np \n",
        "!pip install pydicom\n",
        "import pydicom \n",
        "import matplotlib.pyplot as plt\n",
        "!pip install python-docx\n",
        "import docx \n",
        "import pandas as pd \n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# np.save(r'/content/drive/My Drive/images.npy', images)\n",
        "# np.save(r'/content/drive/My Drive/labels.npy', labels)\n",
        "# np.save(r'/content/drive/My Drive/patient_IDs.npy', patient_IDs)\n",
        "\n",
        "x = np.load(r'/content/drive/My Drive/images.npy')\n",
        "y = np.load(r'/content/drive/My Drive/labels.npy')\n",
        "patient_IDs = np.load(r'/content/drive/My Drive/patient_IDs.npy')\n",
        "\n",
        "## include talking about how our dataset doesn't have enough patients to test on all stages\n",
        "    # only test on the data with enough patients to be learnable from and include a disclaimer \n",
        "# print(len(patient_IDs))"
      ],
      "metadata": {
        "id": "eG1gY2i7wkWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.unique(patient_IDs).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "yrpl1BUdS1cE",
        "outputId": "e129d4f2-65c9-4e0a-a41e-74265a893e1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(61,)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a81089f0ec5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatient_IDs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36munique\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unpack_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indices_to_drop = []\n",
        "\n",
        "for i in range(len(y)):\n",
        "    # print(y)\n",
        "    if y[i] == 'N/A' or y[i] == '4':\n",
        "        indices_to_drop.append(i)\n",
        "        # print('true')\n",
        "\n",
        "print('len x', len(x))\n",
        "x = [x[i] for i in range(len(x)) if i not in indices_to_drop]\n",
        "y = [y[i] for i in range(len(y)) if i not in indices_to_drop]\n",
        "patient_IDs = [patient_IDs[i] for i in range(len(patient_IDs)) if i not in indices_to_drop]\n",
        "# print('len x', len(x))\n",
        "\n",
        "# print(np.unique(y, return_counts=True))\n",
        "# print(np.unique(patient_IDs, return_counts=True))\n",
        "\n",
        "y_new = []\n",
        "\n",
        "for i in range(len(y)):\n",
        "    if y[i] == '1A' or y[i] == '1B':\n",
        "        y_new.append('1')\n",
        "    elif y[i] == '2A' or y[i] == '2B':\n",
        "        y_new.append('2')\n",
        "    elif y[i] == '3A' or y[i] == '3B' or y[i] == '3':\n",
        "        y_new.append('3')\n",
        "    elif y[i] == '4':\n",
        "        y_new.append('4')\n",
        "    else:\n",
        "        print(y_new)\n",
        "    #     y_new.append('N/A')\n",
        "\n",
        "# print('len y', len(y))\n",
        "# print(\"len y_new\", len(y_new))\n",
        "\n",
        "\n",
        "# (array(['1A', '1B', '2A', '2B', '3A', '3B', 'N/A'], dtype='<U3'), array([1230,  419,  301, 1182,  785,  308,  337]))\n",
        "y = y_new\n",
        "\n",
        "\n",
        "# print(np.count(y, return_counts=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ai8pQBkeNaFd",
        "outputId": "8d619c84-47b4-4bc4-cbe8-fee105acfaf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len x 4686\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x), len(y), len(patient_IDs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STrTyldHVYVe",
        "outputId": "175685a6-5102-4dbc-ac0b-bd1ddb347586"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4287 4287 4287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# groups = [int(patient) for patient in data['patient']]\n",
        "groups = []\n",
        "# data[0:0]\n",
        "\n",
        "groups = [int(patient) for patient in patient_IDs]\n",
        "\n",
        "# groups.append\n",
        "# for index, row in data.iterrows():\n",
        "    #  groups.append(int(row['patient']))\n",
        "#     print(row)\n",
        "#     groups.append(int(row['patient']))\n",
        "# groupings by patient ID \n",
        "# please see: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupShuffleSplit.html"
      ],
      "metadata": {
        "id": "vzQEiakKXGHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(groups)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XT1pOBbWYhEh",
        "outputId": "600d819c-0546-4fbe-952f-7672593ca73f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 126, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 141, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 143, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 146, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 157, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 172, 172, 172, 172, 172, 172, 172, 172, 172, 172, 172, 172, 172, 172, 172, 172, 172, 172, 172, 172, 172, 172, 172, 172, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 175, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 208, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 266, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 267, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274, 274]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Grouping code \n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
        "\n",
        "## Create our training, validation, testing datasets (70-15-15 split)\n",
        "# [[list of all patient 1], [patient 2], [etc]] ## labels \n",
        "# [[patient 21], [patient 3]]\n",
        "\n",
        "# 1 label, 1 patient ID, [all of the images]\n",
        "\n",
        "\n",
        "# x = images \n",
        "# y = labels\n",
        "patient_id = patient_IDs\n",
        "\n",
        "# GroupShuffleSplit\n",
        "gss = ShuffleSplit(n_splits=1, train_size=.7, random_state=60)\n",
        "gss.get_n_splits()\n",
        "\n",
        "# train_idx, test_idx = gss.split(x, y, groups)\n",
        "for train_idx, test_idx in gss.split(x, y):\n",
        "    train_idx = train_idx\n",
        "    test_idx = test_idx\n",
        "    # print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n",
        "\n",
        "## Temporarily split into train and test before doing a test/validation split \n",
        "print(len(x), len(train_idx))\n",
        "\n",
        "\n",
        "# x, y, patient_id = np.array(x), np.array(y), np.array(patient_id)\n",
        "print('line 0')\n",
        "x, y, patient_id = np.array(x), np.array(y), np.array(patient_id)\n",
        "print(\"line 1\")\n",
        "x_train = [x[i] for i in train_idx]\n",
        "y_train = [y[i] for i in train_idx]\n",
        "patient_id_train = [patient_id[i] for i in train_idx]\n",
        "print('done')\n",
        "# x_train, y_train, patient_id_train = [x[i] for i in train_idx], [y[i] for i in train_idx], [patient_id[i] for i in train_idx]\n",
        "# x_train = x[train_idx]\n",
        "print(\"line2\")\n",
        "\n",
        "# x_train, y_train, patient_id_train = np.array(x)[train_idx], np.array(y)[train_idx], np.array(patient_id)[train_idx]\n",
        "print('line 3')\n",
        "### gets to this point ^^\n",
        "\n",
        "x_test_temp, y_test_temp, patient_id_test_temp = [x[i] for i in test_idx], [y[i] for i in test_idx], [patient_id[i] for i in test_idx]\n",
        "print(len(x_test_temp))\n",
        "groups_test = [groups[i] for i in test_idx]\n",
        "\n",
        "\n",
        "gss = ShuffleSplit(n_splits=1, train_size=.5, random_state=27) # was 43\n",
        "gss.get_n_splits()\n",
        "\n",
        "for test_idx, valid_idx in gss.split(x_test_temp, y_test_temp):\n",
        "    test_idx = test_idx\n",
        "    valid_idx = valid_idx \n",
        "\n",
        "x_valid, x_test = [list(x_test_temp)[i] for i in valid_idx], [list(x_test_temp)[i] for i in test_idx]\n",
        "y_valid, y_test = [list(y_test_temp)[i] for i in valid_idx], [list(y_test_temp)[i] for i in test_idx]\n",
        "patient_id_valid, patient_id_test = [list(patient_id_test_temp)[i] for i in valid_idx], [list(patient_id_test_temp)[i] for i in test_idx]\n",
        "\n",
        "print(len(x_train), len(x_valid), len(x_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXjW-ozSNWi1",
        "outputId": "21c29e1b-ce56-402f-e48a-3c9ee28196be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "4287 3000\n",
            "line 0\n",
            "line 1\n",
            "done\n",
            "line2\n",
            "line 3\n",
            "1287\n",
            "3000 644 643\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model code"
      ],
      "metadata": {
        "id": "2xsdMrg1GCtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras import Sequential \n",
        "from tensorflow.keras.layers import Conv2D, Activation, MaxPooling2D, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# ^^ transfer learning using resnet 50???"
      ],
      "metadata": {
        "id": "H_LesmL5nKC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 30 # was 30 \n",
        "BS = 16\n",
        "INIT_LR = 1e-4\n",
        "\n",
        "# Beginning: 7, 32, 0.001\n",
        "# End (after tuning): , , "
      ],
      "metadata": {
        "id": "BjcJORK4n4vT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## tf model \n",
        "height, width, depth = 512, 512, 1\n",
        "n_classes = 3\n",
        "model = Sequential()\n",
        "inputShape = (height, width, depth)\n",
        "chanDim = -1\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding=\"same\", input_shape=inputShape, activation='relu'))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), padding=\"same\",activation='relu'))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(3,3)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(n_classes))\n",
        "model.add(Activation(\"softmax\"))"
      ],
      "metadata": {
        "id": "SpXh_yLvSaMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn \n",
        "from sklearn.preprocessing import OneHotEncoder, LabelBinarizer\n",
        "# LabelBinarizer"
      ],
      "metadata": {
        "id": "D6DABq-zE5Jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(y_train)\n",
        "y_train_bin = LabelBinarizer().fit_transform(y_train)\n",
        "y_test_bin = LabelBinarizer().fit_transform(y_test)\n",
        "y_valid_bin = LabelBinarizer().fit_transform(y_valid)\n",
        "y_valid = y_valid_bin\n",
        "y_train = y_train_bin\n",
        "y_test = y_test_bin"
      ],
      "metadata": {
        "id": "CeuH0jq3E_mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.array(x_train, dtype=np.uint8).shape)\n",
        "\n",
        "print(np.unique(y_test))\n",
        "print(np.unique(y_train))\n",
        "\n",
        "x_train = np.array(x_train, dtype=np.uint8)\n",
        "y_train = np.array(y_train)\n",
        "x_test = np.array(x_test, dtype=np.uint8)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kzko8EPYoaWU",
        "outputId": "d03acece-ff16-42fd-e4b9-14a389fd2bdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3000, 512, 512)\n",
            "[0 1]\n",
            "[0 1]\n",
            "(3000, 512, 512) (3000, 3)\n",
            "(643, 512, 512) (643, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = np.concatenate([np.array(x_test), np.array(x_valid)])\n",
        "print(x_test.shape)\n",
        "y_test = np.concatenate([np.array(y_test), np.array(y_valid)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyWP87aeAMcg",
        "outputId": "60cf98c1-fb0b-4887-e055-d8dbd851d207"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1287, 512, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train the network\n",
        "print(\"[INFO] training network...\")\n",
        "model.compile(loss=\"categorical_crossentropy\", \n",
        "              optimizer=Adam(learning_rate=INIT_LR), \n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(x_train, y_train, batch_size=BS,\n",
        "                    validation_data=(np.array(x_test), np.array(y_test)), #was np.array(x_valid)\n",
        "                    epochs=EPOCHS, verbose=1, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0DhedL7n9y1",
        "outputId": "6ab5e6ea-981a-4f96-cf64-974264154e44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] training network...\n",
            "Epoch 1/30\n",
            "188/188 [==============================] - 80s 376ms/step - loss: 25.3002 - accuracy: 0.6430 - val_loss: 26.6929 - val_accuracy: 0.6503\n",
            "Epoch 2/30\n",
            "188/188 [==============================] - 69s 367ms/step - loss: 0.4353 - accuracy: 0.9133 - val_loss: 24.0640 - val_accuracy: 0.7405\n",
            "Epoch 3/30\n",
            "188/188 [==============================] - 69s 365ms/step - loss: 0.1705 - accuracy: 0.9553 - val_loss: 26.9811 - val_accuracy: 0.7234\n",
            "Epoch 4/30\n",
            "188/188 [==============================] - 69s 366ms/step - loss: 0.0949 - accuracy: 0.9763 - val_loss: 33.9501 - val_accuracy: 0.7490\n",
            "Epoch 5/30\n",
            "188/188 [==============================] - 69s 365ms/step - loss: 0.0627 - accuracy: 0.9823 - val_loss: 29.8583 - val_accuracy: 0.7568\n",
            "Epoch 6/30\n",
            "188/188 [==============================] - 69s 367ms/step - loss: 0.0641 - accuracy: 0.9873 - val_loss: 28.1121 - val_accuracy: 0.7521\n",
            "Epoch 7/30\n",
            "188/188 [==============================] - 69s 365ms/step - loss: 0.0392 - accuracy: 0.9880 - val_loss: 29.1829 - val_accuracy: 0.7677\n",
            "Epoch 8/30\n",
            "188/188 [==============================] - 69s 365ms/step - loss: 0.0252 - accuracy: 0.9920 - val_loss: 24.3380 - val_accuracy: 0.7863\n",
            "Epoch 9/30\n",
            "188/188 [==============================] - 69s 369ms/step - loss: 0.0163 - accuracy: 0.9957 - val_loss: 27.1567 - val_accuracy: 0.7801\n",
            "Epoch 10/30\n",
            "188/188 [==============================] - 69s 367ms/step - loss: 0.0436 - accuracy: 0.9897 - val_loss: 25.5710 - val_accuracy: 0.7630\n",
            "Epoch 11/30\n",
            "188/188 [==============================] - 68s 364ms/step - loss: 0.0124 - accuracy: 0.9963 - val_loss: 29.8752 - val_accuracy: 0.7747\n",
            "Epoch 12/30\n",
            "188/188 [==============================] - 69s 365ms/step - loss: 0.0158 - accuracy: 0.9933 - val_loss: 30.2148 - val_accuracy: 0.7941\n",
            "Epoch 13/30\n",
            "188/188 [==============================] - 68s 365ms/step - loss: 0.0091 - accuracy: 0.9973 - val_loss: 38.4808 - val_accuracy: 0.7964\n",
            "Epoch 14/30\n",
            "188/188 [==============================] - 69s 366ms/step - loss: 0.0163 - accuracy: 0.9953 - val_loss: 42.2790 - val_accuracy: 0.8003\n",
            "Epoch 15/30\n",
            "188/188 [==============================] - 68s 365ms/step - loss: 0.0117 - accuracy: 0.9963 - val_loss: 32.7723 - val_accuracy: 0.7918\n",
            "Epoch 16/30\n",
            "188/188 [==============================] - 69s 365ms/step - loss: 0.0115 - accuracy: 0.9960 - val_loss: 35.9945 - val_accuracy: 0.8011\n",
            "Epoch 17/30\n",
            "188/188 [==============================] - 69s 365ms/step - loss: 0.0145 - accuracy: 0.9960 - val_loss: 32.2472 - val_accuracy: 0.7925\n",
            "Epoch 18/30\n",
            "188/188 [==============================] - 69s 366ms/step - loss: 0.0288 - accuracy: 0.9930 - val_loss: 37.4776 - val_accuracy: 0.7933\n",
            "Epoch 19/30\n",
            "188/188 [==============================] - 69s 365ms/step - loss: 0.0093 - accuracy: 0.9963 - val_loss: 25.9631 - val_accuracy: 0.8151\n",
            "Epoch 20/30\n",
            "188/188 [==============================] - 69s 366ms/step - loss: 0.0056 - accuracy: 0.9987 - val_loss: 31.7877 - val_accuracy: 0.8019\n",
            "Epoch 21/30\n",
            "188/188 [==============================] - 69s 366ms/step - loss: 0.0010 - accuracy: 0.9997 - val_loss: 27.6421 - val_accuracy: 0.8112\n",
            "Epoch 22/30\n",
            "188/188 [==============================] - 69s 367ms/step - loss: 0.0013 - accuracy: 0.9993 - val_loss: 29.1191 - val_accuracy: 0.8011\n",
            "Epoch 23/30\n",
            "188/188 [==============================] - 69s 365ms/step - loss: 0.0083 - accuracy: 0.9973 - val_loss: 25.3035 - val_accuracy: 0.7902\n",
            "Epoch 24/30\n",
            "188/188 [==============================] - 69s 366ms/step - loss: 0.0127 - accuracy: 0.9973 - val_loss: 24.3555 - val_accuracy: 0.8081\n",
            "Epoch 25/30\n",
            "188/188 [==============================] - 69s 366ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 24.6008 - val_accuracy: 0.7949\n",
            "Epoch 26/30\n",
            "188/188 [==============================] - 69s 368ms/step - loss: 8.5311e-04 - accuracy: 0.9997 - val_loss: 26.9317 - val_accuracy: 0.7949\n",
            "Epoch 27/30\n",
            "188/188 [==============================] - 69s 368ms/step - loss: 0.0017 - accuracy: 0.9993 - val_loss: 22.5554 - val_accuracy: 0.8057\n",
            "Epoch 28/30\n",
            "188/188 [==============================] - 69s 369ms/step - loss: 0.0324 - accuracy: 0.9937 - val_loss: 23.0349 - val_accuracy: 0.8050\n",
            "Epoch 29/30\n",
            "188/188 [==============================] - 69s 366ms/step - loss: 0.0049 - accuracy: 0.9977 - val_loss: 26.3149 - val_accuracy: 0.8244\n",
            "Epoch 30/30\n",
            "188/188 [==============================] - 69s 367ms/step - loss: 0.0048 - accuracy: 0.9973 - val_loss: 36.4762 - val_accuracy: 0.7995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds = model.predict(np.array(x_test))\n",
        "test_preds = np.argmax(test_preds, axis=1)\n",
        "y_test_calc = np.argmax(y_test, axis=1)\n",
        "print(test_preds)\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Test accuracy: \", accuracy_score(y_test_calc, test_preds))\n"
      ],
      "metadata": {
        "id": "Rqs1MLdgmVE-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d90fe2f6-da52-416a-8771-11ede8db3c16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 2 0 ... 0 2 2]\n",
            "Test accuracy:  0.7995337995337995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RghuRrqrHJsJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}